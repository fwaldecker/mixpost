# AI-Powered Video Clipping Platform - Complete Architecture Plan

## Project Overview
Build a comprehensive AI-powered video clipping platform that combines the capabilities of Opus Clips with enhanced recording features similar to Riverside.fm, fully integrated with the existing MixPost social media scheduler.

## Key Requirements & Goals

### Core Features
- **AI-Powered Clipping**: Intelligent identification and extraction of viral-worthy video segments
- **Custom Prompt System**: User-controllable prompts for different content types
- **Multi-Stream Recording**: Native support for multiple video sources (participants, screen share)
- **Simple Video Editor**: Post-clip editing with text overlays and trimming
- **MixPost Integration**: Direct scheduling to social media platforms
- **Future Recording Platform**: WebRTC-based recording similar to Riverside.fm

### Differentiators from Opus Clips
1. **User-Controlled Prompts**: Full customization of AI prompts for clip selection
2. **Multi-Source Support**: Handle Zoom recordings, streaming services, multiple video tracks
3. **Higher Quality Output**: Combine screen share and active speaker for professional results
4. **Direct Social Integration**: Native integration with MixPost for scheduling
5. **Self-Hosted Option**: Complete control over data and processing

---

## Technical Architecture

### Technology Stack

#### Backend Infrastructure
- **Primary Framework**: Node.js with Express/Fastify or Python with FastAPI
- **Queue System**: Redis + Bull Queue for asynchronous job processing
- **Database**: 
  - PostgreSQL for structured data (users, clips, metadata)
  - MongoDB for flexible content storage (prompts, templates)
- **Cache Layer**: Redis for session management and caching
- **Video Processing**: FFmpeg (core) with fluent-ffmpeg wrapper
- **Storage**: S3-compatible object storage (S3/MinIO/Cloudflare R2) for video files
  - Cloudflare R2 recommended for no egress fees

#### Frontend Stack
- **Framework**: Next.js 14+ with React 18
- **Video Editor**: 
  - Remotion framework for programmatic video editing (server-side rendering, real-time preview)
  - Etro.js as alternative for lighter editing tasks (TypeScript-based, hardware-accelerated)
- **Player**: Video.js for playback
- **Audio Visualization**: Wavesurfer.js for waveform display
- **Canvas Editing**: Fabric.js or Konva.js for overlay editing
- **Real-time Updates**: Socket.io for live progress updates
- **UI Framework**: Tailwind CSS + shadcn/ui components
- **State Management**: Zustand or Redux Toolkit

#### AI/ML Services
- **Transcription**: 
  - OpenAI Whisper (local or API) for base transcription
  - WhisperX for improved word-level timestamps and alignment
- **Speaker Diarization**: Pyannote for identifying different speakers
- **Content Analysis**: 
  - LLaMA 3.1 (self-hosted) for cost-effective processing
  - Claude API or GPT-4 for premium features
- **Prompt Management**: Langchain for LLM orchestration

#### Recording Infrastructure
- **WebRTC Stack**: 
  - OpenVidu (recommended) for enterprise-grade recording with individual streams
  - MediaSoup as alternative for custom requirements
  - MediaRecorder API for browser recording fallback
  - Simple-peer for P2P connections
- **Formats**: WebM (recording) â†’ MP4 (processing)
- **Quality**: Support 480p to 4K recording
- **Features**: Progressive upload, automatic reconnection, quality monitoring

---

## Workflow Engine Architecture (N8N Replacement)

### Core Workflow Patterns from Analysis

Based on your N8N workflows, the system needs to handle:

1. **Content Creation Pipeline** (Everyday Creator)
   - Webhook triggers from external systems
   - Multi-step video processing (crop, caption, style)
   - Template-based content generation (Placid API)
   - Google Drive integration for storage
   - Airtable as content database

2. **AI Copywriting** (Everyday Copywriter)
   - LLM integration for content generation
   - Dynamic prompt templates
   - Content type classification
   - Batch processing capabilities

3. **Video Processing** (Crop & Caption Production)
   - AI-powered face/screen detection
   - Dynamic crop calculations
   - Multi-style caption generation
   - Color palette analysis
   - FFmpeg compose operations

4. **RAG Ingestion** (Knowledge Base)
   - PDF text extraction
   - Document chunking and embedding
   - Supabase vector storage
   - Session-based retrieval

### Proposed Workflow Engine Design

#### 1. Workflow Definition System

```typescript
interface WorkflowDefinition {
  id: string;
  name: string;
  trigger: TriggerConfig;
  nodes: WorkflowNode[];
  connections: NodeConnection[];
  variables: WorkflowVariable[];
  errorHandling: ErrorStrategy;
}

interface WorkflowNode {
  id: string;
  type: NodeType; // 'webhook', 'ai', 'transform', 'api', 'condition', 'loop'
  config: NodeConfig;
  inputs: NodeInput[];
  outputs: NodeOutput[];
  retryPolicy?: RetryConfig;
}
```

#### 2. Core Components

**Workflow Runtime Engine**
- **Execution Engine**: Node.js-based with Bull Queue for job processing
- **State Management**: Redis for workflow state and variables
- **Event System**: EventEmitter pattern for node communication
- **Error Handling**: Circuit breaker pattern with exponential backoff

**Node Types Library**
```javascript
// Base node types matching your N8N patterns
const nodeTypes = {
  // Triggers
  webhook: WebhookTriggerNode,
  schedule: ScheduleTriggerNode,
  queue: QueueTriggerNode,
  
  // Processing
  ai: {
    openai: OpenAINode,
    claude: ClaudeNode,
    whisper: WhisperNode,
    customPrompt: CustomPromptNode
  },
  
  // Video Operations
  video: {
    crop: VideoCropNode,
    caption: VideoCaptionNode,
    compose: VideoComposeNode,
    extract: VideoExtractNode,
    analyze: VideoAnalyzeNode
  },
  
  // Data Operations
  data: {
    transform: DataTransformNode,
    aggregate: AggregateNode,
    split: SplitNode,
    merge: MergeNode
  },
  
  // Integrations
  integration: {
    airtable: AirtableNode,
    googleDrive: GoogleDriveNode,
    supabase: SupabaseNode,
    mixpost: MixpostNode,
    placid: PlacidNode
  },
  
  // Control Flow
  control: {
    if: ConditionalNode,
    switch: SwitchNode,
    loop: LoopNode,
    wait: WaitNode,
    parallel: ParallelNode
  }
};
```

#### 3. Visual Workflow Builder

**Frontend Components**
- **React Flow**: For visual workflow design (similar to N8N)
- **Node Palette**: Drag-and-drop node library
- **Connection Manager**: Visual edge connections
- **Property Panel**: Node configuration UI
- **Debug Panel**: Real-time execution monitoring

**Key Features**
```typescript
interface WorkflowBuilder {
  // Visual editing
  addNode(type: NodeType, position: Position): Node;
  connectNodes(source: NodeId, target: NodeId): Connection;
  configureNode(nodeId: NodeId, config: NodeConfig): void;
  
  // Testing
  testNode(nodeId: NodeId, sampleData: any): TestResult;
  debugWorkflow(input: any): DebugTrace;
  
  // Versioning
  saveVersion(description: string): Version;
  compareVersions(v1: Version, v2: Version): Diff;
  rollback(versionId: string): void;
}
```

#### 4. Webhook System

**Webhook Manager**
```typescript
class WebhookManager {
  // Registration
  registerWebhook(workflow: WorkflowId, config: WebhookConfig): WebhookEndpoint;
  
  // Security
  validateSignature(request: Request, secret: string): boolean;
  rateLimit(endpoint: string, limits: RateLimitConfig): void;
  
  // Processing
  async handleWebhook(request: Request): WorkflowExecution;
  
  // Monitoring
  trackWebhook(execution: Execution): Analytics;
}
```

**External Integration Support**
- **N8N Compatibility**: Webhook format compatibility
- **Zapier/Make**: Standard webhook patterns
- **Custom Headers**: Signature validation
- **Retry Logic**: Automatic retry with backoff

### Specific Workflow Implementations

#### 1. Video Clipping Workflow
```yaml
name: "AI Video Clipper"
trigger:
  type: webhook
  path: /clip/process
  
nodes:
  - id: extract_video
    type: video/extract
    config:
      source: webhook.video_url
      
  - id: transcribe
    type: ai/whisper
    input: extract_video.audio
    
  - id: detect_scenes
    type: video/analyze
    config:
      method: scenedetect
      threshold: 0.3
      
  - id: generate_clips
    type: ai/customPrompt
    inputs:
      transcript: transcribe.text
      scenes: detect_scenes.scenes
    config:
      prompt: "Identify viral moments..."
      
  - id: process_clips
    type: parallel
    forEach: generate_clips.clips
    nodes:
      - crop_video
      - add_captions
      - optimize_format
      
  - id: schedule_posts
    type: integration/mixpost
    input: process_clips.output
```

#### 2. Content Creation Workflow
```yaml
name: "Everyday Creator"
trigger:
  type: airtable
  table: content_calendar
  
nodes:
  - id: fetch_content
    type: integration/airtable
    
  - id: analyze_content_type
    type: ai/classify
    
  - id: switch_processor
    type: control/switch
    cases:
      video: video_pipeline
      image: image_pipeline
      text: text_pipeline
      
  - id: generate_captions
    type: ai/copywriter
    config:
      style: brand_voice
      platform: multi
      
  - id: publish
    type: integration/mixpost
```

#### 3. RAG Ingestion Workflow
```yaml
name: "Knowledge Base Updater"
trigger:
  type: webhook
  path: /rag/ingest
  
nodes:
  - id: extract_text
    type: document/extract
    config:
      formats: [pdf, docx, txt]
      
  - id: chunk_text
    type: text/splitter
    config:
      method: recursive_character
      chunk_size: 1000
      overlap: 200
      
  - id: generate_embeddings
    type: ai/embeddings
    config:
      model: text-embedding-3-small
      
  - id: store_vectors
    type: integration/supabase
    config:
      table: documents
      upsert: true
```

### Database Schema for Workflows

```sql
-- Workflow definitions
CREATE TABLE workflows (
    id UUID PRIMARY KEY,
    name VARCHAR(255),
    description TEXT,
    definition JSONB,
    version INT,
    status VARCHAR(50),
    created_by UUID,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);

-- Workflow executions
CREATE TABLE workflow_executions (
    id UUID PRIMARY KEY,
    workflow_id UUID REFERENCES workflows(id),
    trigger_type VARCHAR(50),
    trigger_data JSONB,
    status VARCHAR(50),
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error TEXT,
    result JSONB
);

-- Node executions
CREATE TABLE node_executions (
    id UUID PRIMARY KEY,
    execution_id UUID REFERENCES workflow_executions(id),
    node_id VARCHAR(255),
    status VARCHAR(50),
    input_data JSONB,
    output_data JSONB,
    error TEXT,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    duration_ms INT
);

-- Webhook endpoints
CREATE TABLE webhook_endpoints (
    id UUID PRIMARY KEY,
    workflow_id UUID REFERENCES workflows(id),
    path VARCHAR(255) UNIQUE,
    method VARCHAR(10),
    headers JSONB,
    secret VARCHAR(255),
    active BOOLEAN,
    created_at TIMESTAMP
);
```

### Migration Strategy from N8N

1. **Workflow Import Tool**
   - Parse N8N JSON exports
   - Map N8N nodes to native nodes
   - Convert expressions and variables
   - Maintain webhook compatibility

2. **Dual-Run Period**
   - Run both systems in parallel
   - Compare outputs for validation
   - Gradual migration by workflow type
   - Rollback capability

3. **Feature Parity Checklist**
   - [ ] Webhook triggers with authentication
   - [ ] Conditional branching
   - [ ] Loop and iteration
   - [ ] Error handling and retry
   - [ ] Variable management
   - [ ] Expression evaluation
   - [ ] External API calls
   - [ ] File operations
   - [ ] Database operations

## System Components

### 1. Recording Module

#### Features
- **Local Recording**: Each participant records locally for highest quality
- **Multi-Track Support**: Separate tracks for each participant and screen share
- **Automatic Backup**: Progressive upload during recording
- **Resilience**: Continue recording even with connection issues

#### Technical Implementation
```javascript
// Core recording architecture
- MediaRecorder API for browser-based recording
- IndexedDB for local storage buffer
- WebSocket for signaling and control
- Progressive upload with resumable uploads
```

### 2. AI Clipping Engine

#### Processing Pipeline
1. **Input Stage**
   - Accept multiple video formats (MP4, MOV, WebM, MKV)
   - Handle multi-track recordings from Zoom/Teams
   - Support direct streaming service connections
   - Zoom API integration for cloud recordings (`GET /users/{userId}/recordings`)
   - Google Drive API for video file selection
   - YouTube Live archive support

2. **Transcription Stage**
   - Whisper model for accurate transcription with word-level timestamps
   - Alternative: Alibaba's Paraformer ASR (from FunClip)
   - Speaker diarization using NVIDIA NeMo or CAM++
   - Support for existing transcripts (Zoom, YouTube captions)
   - Output formats: SRT, VTT, structured JSON

3. **Analysis Stage**
   - LLM processes transcript with custom prompts
   - Multi-modal analysis (future):
     - Audio cues (volume, pitch changes)
     - Visual cues (slide transitions, scene changes)
     - Sentiment analysis from text
   - Chunking strategy for long transcripts:
     - Topic segmentation using embeddings
     - Summarization before highlight selection
   - Identify key moments based on:
     - User-defined criteria via prompts
     - Engagement patterns
     - Topic changes
     - Emotional peaks
     - Visual interest points

4. **Clipping Stage**
   - Extract segments based on AI recommendations
   - Multi-segment editing (combine non-contiguous clips)
   - Generate multiple duration variants (15s, 30s, 60s)
   - Apply platform-specific optimizations
   - Maintain Edit Decision List (EDL) for precise cuts

#### Prompt Engineering System

**LLM-Based Clip Selection (FunClip-inspired)**
```python
# Based on FunClip's approach
class ClipSelector:
    def __init__(self):
        self.default_prompts = {
            "viral": "Identify the most engaging, surprising, or emotionally impactful moments that would make viewers want to share",
            "educational": "Find segments where key concepts, tips, or insights are clearly explained",
            "funny": "Locate humorous moments, jokes, or unexpected funny situations",
            "highlights": "Extract the most important or climactic moments from the content"
        }
    
    def analyze_transcript(self, transcript, user_prompt=None):
        # Combine default template with user customization
        prompt = f"""
        Analyze this video transcript and identify the best segments for short clips.
        Focus on: {user_prompt or self.default_prompts['viral']}
        
        For each recommended clip:
        1. Provide exact start and end timestamps
        2. Give a compelling title (5-10 words)
        3. Explain why this segment is valuable
        4. Rate its potential (1-10)
        
        Transcript: {transcript}
        """
        return self.llm.generate(prompt)
```

**User Prompt Customization**
```yaml
prompt_templates:
  base_template: |
    Analyze the video and find {clip_count} clips that {user_criteria}.
    Each clip should be {duration} seconds long.
    Prioritize segments that {engagement_factor}.
  
  user_variables:
    clip_count: 3-10
    user_criteria: "custom focus area"
    duration: 15-60
    engagement_factor: "would stop scrolling"
  
  preset_styles:
    alex_hormozi: "Find powerful business insights with high energy delivery"
    educational: "Identify clear teaching moments with actionable takeaways"
    storytelling: "Locate compelling narrative arcs with emotional hooks"
    case_study: "Extract specific examples, data, or proof points"
```

### 3. Video Editor Interface

#### Core Features
- **Timeline Editor**: 
  - Visual timeline with multi-track support
  - Waveform visualization (WaveSurfer.js)
  - Frame-accurate trimming with handles
  - Text-based editing (Descript-style)
  - Delete words from transcript to cut video

- **Video Composition & Layout**:
  - Drag-and-drop repositioning of video layers
  - Picture-in-picture presets
  - Side-by-side layouts
  - Automatic face-centered cropping
  - Custom layouts via FFmpeg filters:
    ```bash
    # Example: Vertical format with screen + speaker
    [screen]scale=1080:1440[bg];
    [speaker]scale=300:300[fg];
    [bg][fg]overlay=700:1400[out]
    ```
  - Smart reframing for aspect ratios (16:9 â†’ 9:16)
  - Face detection for auto-centering

- **Text Overlays & Graphics**: 
  - Auto-generated captions with word highlighting
  - Intro/outro text cards
  - Custom text with animations
  - Brand templates and presets
  - Copy-paste text functionality
  - FFmpeg drawtext filter implementation

- **Transitions & Effects**: 
  - Preset transitions between clips
  - Cross-fade for smooth audio cuts
  - Jump-cut optimization for fast-paced content

- **Audio Processing**: 
  - Background music library
  - Audio ducking for voiceovers
  - Noise reduction
  - Multi-track audio mixing
  - Crossfade between cuts

- **Export Presets**: 
  - Platform-specific optimizations:
    - TikTok: 9:16, 60fps, H.264
    - Instagram Reels: 9:16, 30fps
    - YouTube Shorts: 9:16, up to 60s
    - LinkedIn: 16:9 or 1:1
  - Codec settings for quality vs size

#### Implementation with Remotion
```typescript
// Remotion-based video composition
const VideoComposition = () => {
  return (
    <Composition
      id="main"
      component={VideoClip}
      durationInFrames={1800} // 60 seconds at 30fps
      fps={30}
      width={1920}
      height={1080}
      defaultProps={{
        clips: [],
        overlays: [],
        transitions: []
      }}
    />
  );
};
```

### 4. MixPost Integration

#### Integration Points
1. **Media Library**: Direct upload of processed clips
2. **Post Creation**: API endpoints for creating scheduled posts
3. **Analytics**: Performance tracking through MixPost metrics
4. **Webhooks**: Real-time updates on post status

#### API Integration
```javascript
// MixPost API integration example
const scheduleToMixPost = async (clipData) => {
  const response = await fetch('/api/workspace/{workspace}/posts', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${API_TOKEN}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      content: clipData.caption,
      media_ids: [clipData.mediaId],
      scheduled_at: clipData.scheduledTime,
      accounts: clipData.selectedAccounts
    })
  });
};
```

---

## Implementation Roadmap

### Milestone 1: MVP Foundation
**Goal**: Basic upload, process, and clip functionality

**Core Infrastructure**
- [ ] Initialize backend (Node.js/Express or Python/FastAPI)
- [ ] Set up PostgreSQL and Redis
- [ ] Configure FFmpeg processing pipeline
- [ ] Create basic REST API structure

**Basic Features**
- [ ] File upload system with chunked uploads
- [ ] Whisper transcription integration
- [ ] Basic scene detection using PySceneDetect
- [ ] Simple web interface with Next.js
- [ ] MixPost API connection for scheduling

**Deliverable**: Working prototype that can upload video, transcribe, detect scenes, and create basic clips

### Milestone 2: AI Intelligence Layer
**Goal**: Smart clipping with customizable prompts

**LLM Integration**
- [ ] Integrate LLaMA 3.1 (self-hosted) or Claude/GPT-4 API
- [ ] Create prompt template system with categories
- [ ] Build prompt customization UI
- [ ] Implement clip scoring and ranking algorithm

**Enhanced Processing**
- [ ] Multi-format clip generation (9:16, 1:1, 16:9)
- [ ] Auto-caption generation with styling options
- [ ] Platform-specific optimizations
- [ ] Batch processing system with queue management

**Deliverable**: AI-powered clipping with user-controlled prompts and intelligent content analysis

### Milestone 3: Recording Platform
**Goal**: WebRTC-based multi-stream recording system

**Recording Infrastructure**
- [ ] WebRTC signaling server setup
- [ ] MediaRecorder API implementation
- [ ] Multi-participant support with separate tracks
- [ ] Local storage with IndexedDB fallback

**Recording Features**
- [ ] Screen sharing with audio support
- [ ] Recording UI with real-time preview
- [ ] Progressive upload system with resumable uploads
- [ ] Recording session management dashboard

**Deliverable**: Full recording platform with multi-track support similar to Riverside.fm

### Milestone 4: Advanced Video Editor
**Goal**: Professional browser-based video editing capabilities

**Editor Core**
- [ ] Remotion or Fabric.js video editor integration
- [ ] Timeline interface with multi-track support
- [ ] Text overlay system with animations
- [ ] Transition effects library

**Advanced Features**
- [ ] Audio editing tools (ducking, noise reduction)
- [ ] Export presets for each platform
- [ ] Template system for consistent branding
- [ ] Real-time collaboration features

**Deliverable**: Complete video editing suite integrated with the platform

### Milestone 5: Comprehensive Analytics & Ad Platform Integration
**Goal**: Build a unified analytics dashboard and ad campaign management system

**Phase 1: Analytics Infrastructure**
- [ ] Implement data pipeline for social media metrics
  - Platform-specific API integrations
  - Real-time data ingestion with Apache Kafka/Pulsar
  - Time-series database (TimescaleDB or InfluxDB)
  - Data warehouse (ClickHouse or Apache Druid)
- [ ] Build analytics aggregation layer
  - Cross-platform metric normalization
  - Parent video to clips relationship tracking
  - Engagement rate calculations
  - ROI and attribution modeling
- [ ] Create analytics API
  - GraphQL endpoint for flexible queries
  - REST API for standard reports
  - WebSocket for real-time updates
  - Batch export capabilities

**Phase 2: Analytics Dashboard Development**
- [ ] Select and implement dashboard framework
  - **Metabase** for user-friendly self-service analytics
  - **Apache Superset** for advanced visualizations
  - **Grafana** for real-time monitoring
  - Custom React components for specialized views
- [ ] Build core analytics views
  - Platform breakdown (views, engagement by platform)
  - Clip performance metrics
  - Parent video attribution dashboard
  - Trend analysis and forecasting
  - Audience demographics and insights
- [ ] Implement advanced analytics features
  - Cohort analysis
  - Funnel visualization
  - A/B test tracking
  - Predictive performance modeling
  - Sentiment analysis integration

**Phase 3: Social Media Analytics Integration**
- [ ] Implement native platform analytics APIs
  - **Facebook/Instagram Insights API**
    - Post insights (reach, impressions, engagement)
    - Video metrics (views, watch time, retention)
    - Audience demographics
    - Story analytics
  - **Twitter/X Analytics API**
    - Tweet impressions and engagements
    - Video view metrics
    - Follower analytics
  - **LinkedIn Analytics API**
    - Post performance metrics
    - Follower demographics
    - Company page analytics
  - **TikTok Analytics API**
    - Video views and engagement
    - Follower growth
    - Trending performance
  - **YouTube Analytics API**
    - Watch time and retention
    - Traffic sources
    - Revenue metrics
- [ ] Build unified metrics pipeline
  - Standardize metrics across platforms
  - Handle rate limiting and quotas
  - Implement retry logic and error handling
  - Cache frequently accessed data

**Phase 4: Ad Campaign Management System**
- [ ] Implement ad platform integrations
  - **Meta (Facebook/Instagram) Ads**
    - Official Business SDK integration
    - Campaign creation and management
    - Audience targeting and lookalike audiences
    - Creative optimization
    - Budget management and bidding
  - **Google Ads**
    - Official client library integration
    - Search, Display, and YouTube campaigns
    - Keyword research and optimization
    - Shopping campaigns for e-commerce
  - **TikTok Ads**
    - Marketing API integration
    - Spark Ads from organic content
    - Creative tools integration
    - Audience targeting
  - **LinkedIn Ads**
    - Campaign Manager API
    - Sponsored content and messaging
    - Lead generation forms
    - Account-based marketing
- [ ] Build unified ad management interface
  - Cross-platform campaign creation
  - Budget allocation and optimization
  - A/B testing framework
  - Creative asset management
  - Approval workflows

**Phase 5: Advanced Ad Features**
- [ ] Implement ad automation
  - Rule-based campaign optimization
  - Automatic bid adjustments
  - Budget reallocation based on performance
  - Creative rotation and testing
  - Dayparting and scheduling
- [ ] Build conversion tracking
  - Pixel implementation and management
  - Cross-platform attribution
  - Custom conversion events
  - ROAS calculation and optimization
- [ ] Create reporting and insights
  - Cross-platform ad performance dashboard
  - Creative performance analysis
  - Audience insights and overlap
  - Competitive analysis
  - Export capabilities for stakeholders

**Deliverable**: Fully integrated analytics and advertising platform with comprehensive tracking and optimization capabilities

### Milestone 6: Native Social Media Scheduler (MixPost Evolution)
**Goal**: Transform MixPost into a fully integrated, modern social media management system

**Phase 1: MixPost Codebase Analysis**
- [ ] Deep dive into MixPost Laravel package structure
- [ ] Document all social provider integrations and APIs
- [ ] Map database schema and relationships
- [ ] Identify reusable components and services
- [ ] Extract core scheduling and publishing logic

**Phase 2: Architecture Migration**
- [ ] Port MixPost PHP services to Node.js/TypeScript
  - Social provider abstractions
  - OAuth authentication flows
  - Media handling pipelines
  - Post scheduling engine
- [ ] Modernize database schema
  - Migrate from Laravel migrations to Prisma/TypeORM
  - Optimize for time-series data
  - Add support for advanced analytics
- [ ] Rebuild queue system
  - Replace Laravel Horizon with Bull/BullMQ
  - Implement distributed job processing
  - Add real-time status updates

**Phase 3: UI/UX Transformation**
- [ ] Design new interface with modern design system
  - Calendar view with drag-and-drop
  - Kanban board for content pipeline
  - Real-time preview for all platforms
  - Advanced analytics dashboard
- [ ] Build React components library
  - Post composer with rich text editor
  - Media gallery with AI tagging
  - Platform-specific previews
  - Bulk scheduling interface
- [ ] Mobile-first responsive design
  - Progressive Web App (PWA) support
  - Native mobile app considerations
  - Touch-optimized interactions

**Phase 4: Enhanced Features Beyond MixPost**
- [ ] AI-Powered Content Generation
  - Caption suggestions based on video content
  - Hashtag recommendations
  - Optimal posting time predictions
  - A/B testing automation
- [ ] Advanced Analytics
  - Cross-platform performance tracking
  - Competitor analysis
  - Sentiment analysis
  - ROI tracking and attribution
- [ ] Team Collaboration
  - Role-based access control
  - Approval workflows
  - Content calendar sharing
  - Comments and annotations
- [ ] Content Intelligence
  - Trend detection and alerts
  - Viral moment predictions
  - Content performance forecasting
  - Automated content recycling
- [ ] AI Comment System for Post Refinement
  - Natural language commands for editing
  - AI agent with modification tools
  - Real-time preview updates
  - Version history tracking
- [ ] Smart Lists with Auto-Routing
  - Content-aware platform selection
  - Automatic scheduling optimization
  - Bulk review and approval interface
  - Intelligent time slot distribution

**Deliverable**: Fully integrated social media management platform that replaces MixPost

---

## Technical Specifications

### Video Processing Pipeline

```mermaid
graph LR
    A[Input Video] --> B[Demux & Decode]
    B --> C[Audio Extraction]
    C --> D[Whisper Transcription]
    D --> E[LLM Analysis]
    E --> F[Clip Identification]
    F --> G[Segment Extraction]
    G --> H[Post-Processing]
    H --> I[Multi-Format Export]
    I --> J[MixPost Scheduling]
```

### Database Schema (Simplified)

```sql
-- Core tables structure
CREATE TABLE projects (
    id UUID PRIMARY KEY,
    user_id UUID NOT NULL,
    name VARCHAR(255),
    source_video_url TEXT,
    status VARCHAR(50),
    created_at TIMESTAMP
);

CREATE TABLE clips (
    id UUID PRIMARY KEY,
    project_id UUID REFERENCES projects(id),
    start_time FLOAT,
    end_time FLOAT,
    transcript TEXT,
    score FLOAT,
    metadata JSONB
);

CREATE TABLE prompts (
    id UUID PRIMARY KEY,
    user_id UUID,
    name VARCHAR(255),
    category VARCHAR(100),
    prompt_template TEXT,
    variables JSONB
);
```

### API Endpoints Structure

```yaml
/api/v1:
  /projects:
    POST: Create new project
    GET: List projects
    /{id}:
      GET: Get project details
      PUT: Update project
      DELETE: Delete project
  
  /clips:
    POST: Generate clips
    GET: List clips
    /{id}:
      GET: Get clip details
      PUT: Edit clip
      POST /export: Export clip
  
  /recording:
    POST /session: Start recording session
    GET /session/{id}: Get session status
    POST /upload: Upload recording chunk
  
  /prompts:
    GET: List prompts
    POST: Create custom prompt
    PUT /{id}: Update prompt
```

---

## Infrastructure & DevOps

### Deployment Architecture

```yaml
services:
  frontend:
    technology: Next.js
    hosting: Vercel or self-hosted
    scaling: Horizontal with load balancer
  
  backend:
    technology: Node.js/Python
    hosting: Docker containers on AWS/GCP
    scaling: Auto-scaling based on queue depth
  
  processing:
    technology: FFmpeg workers
    hosting: GPU-enabled instances for AI
    scaling: Queue-based auto-scaling
  
  storage:
    primary: S3-compatible object storage
    cache: CloudFront CDN
    database: Managed PostgreSQL
```

### Monitoring & Observability

- **Metrics**: Prometheus + Grafana
- **Logging**: ELK Stack or CloudWatch
- **APM**: New Relic or DataDog
- **Error Tracking**: Sentry

---

## Cost Optimization Strategies

### Compute Optimization
1. **Spot Instances**: Use for non-critical processing
2. **Reserved Instances**: For predictable baseline load
3. **Serverless**: Lambda for lightweight operations

### AI/ML Cost Reduction
1. **Local Models**: Self-host Whisper and LLaMA
2. **Batching**: Process multiple clips together
3. **Caching**: Store and reuse common analysis results
4. **Tiered Service**: Premium features use better models

### Storage Optimization
1. **Lifecycle Policies**: Move old content to cold storage
2. **Compression**: Optimize video formats
3. **CDN Caching**: Reduce origin bandwidth
4. **Cleanup Jobs**: Remove temporary files regularly

---

## Security Considerations

### Data Protection
- **Encryption**: At rest and in transit
- **Access Control**: Role-based permissions
- **API Security**: Rate limiting and authentication
- **Content Validation**: Virus scanning and format validation

### Privacy Compliance
- **GDPR**: Data deletion and export capabilities
- **CCPA**: California privacy requirements
- **Data Retention**: Configurable retention policies
- **User Consent**: Clear privacy policies

---

## Integration Examples

### Zoom Integration
```javascript
// Zoom webhook for automatic processing
app.post('/webhooks/zoom', async (req, res) => {
  const { event, payload } = req.body;
  
  if (event === 'recording.completed') {
    const { download_urls } = payload;
    
    // Queue for processing
    await processQueue.add('zoom-recording', {
      urls: download_urls,
      meetingId: payload.meeting_id,
      userId: payload.host_id
    });
  }
});
```

### Streaming Platform Connection
```python
# Example: YouTube Live stream processing
from youtube_dl import YoutubeDL

def download_stream(url, duration=3600):
    ydl_opts = {
        'format': 'best',
        'outtmpl': 'downloads/%(title)s.%(ext)s',
        'duration': duration
    }
    
    with YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])
```

---

## Performance Benchmarks

### Target Metrics
- **Upload Speed**: 100+ Mbps sustained
- **Processing Time**: < 5 minutes for 1-hour video
- **Transcription Accuracy**: > 95% with Whisper
- **Clip Generation**: < 30 seconds per clip
- **API Response Time**: < 200ms p95

### Scalability Targets
- **Concurrent Users**: 10,000+
- **Daily Videos**: 100,000+
- **Storage**: Petabyte-scale capability
- **Geographic Distribution**: Global CDN presence

---

## Competitive Analysis

### vs Opus Clips
**Advantages**:
- Custom prompts
- Multi-source recording
- Direct social scheduling
- Self-hosted option

**Challenges**:
- Building AI accuracy
- Marketing reach
- Feature parity

### vs Riverside.fm
**Advantages**:
- Integrated clipping
- API access
- Custom workflows

**Challenges**:
- Recording stability
- Browser compatibility
- Network resilience

---

## Success Metrics

### Technical KPIs
- System uptime > 99.9%
- Processing success rate > 95%
- API availability > 99.95%
- Average processing time < 5 min

### Business KPIs
- User activation rate
- Clips generated per user
- Social media engagement rates
- Platform retention metrics

---

## Lessons from Existing Solutions

### Key Insights from Research

#### From OpusClip
- **ClipAnything Feature**: Users type prompts to guide clipping
- **Multi-segment Combination**: Can combine non-contiguous segments
- **Visual Polish**: Animated captions with highlighted keywords
- **Auto-centering**: AI ensures speaker stays centered when cropping
- **Viral Optimization**: Analyzes social trends for clip selection

#### From Riverside.fm
- **Local Recording**: Each participant records locally for quality
- **Progressive Upload**: Continuous backup during recording
- **Magic Clips**: AI-powered highlight detection
- **Text-Based Editing**: Edit video by editing transcript
- **Multi-track Sync**: Separate tracks remain synchronized

#### From FunClip (Alibaba)
- **Open-Source Proof**: Shows feasibility with open tools
- **LLM Integration**: Uses prompts for intelligent clipping
- **Speaker Diarization**: Identifies different speakers
- **Multi-segment Support**: Can select multiple quotes
- **Local Processing**: Everything runs on-premise

#### From Descript
- **Paper Editing**: Text document as primary interface
- **Word-Level Precision**: Delete specific words from video
- **Filler Word Removal**: Automatic "um" detection
- **Overdub**: AI voice for corrections

### Our Unique Value Propositions

1. **User-Controlled AI**: Full prompt customization vs black-box automation
2. **Multi-Source Quality**: Handle separate tracks for professional output
3. **Workflow Integration**: Direct MixPost scheduling vs manual export
4. **Self-Hosted Control**: Data sovereignty and customization
5. **API-First Design**: Programmatic access for agency workflows
6. **Batch Processing**: Generate week's content from one video
7. **Content Packages**: Video + text + image generation

## Implementation Priorities

### Critical Path Items
1. **Transcription Accuracy**: Foundation for everything else
2. **LLM Prompt Refinement**: Quality of clip selection
3. **FFmpeg Pipeline**: Reliable video processing
4. **Timeline Sync**: Text-to-video alignment
5. **Multi-track Handling**: Differentiator feature

### Performance Targets
- Transcription: < 0.2x real-time with GPU
- Clip Generation: < 30s per clip
- Preview Rendering: Real-time playback
- Final Export: < 1x real-time
- Concurrent Users: 10+ for MVP

### Technical Debt to Avoid
- Don't hardcode aspect ratios
- Abstract video processing pipeline
- Maintain clean API boundaries
- Document FFmpeg filter chains
- Version control for prompts

## MixPost Migration Strategy

### Understanding MixPost Architecture

#### MixPost Features to Leverage
Based on the codebase analysis, MixPost provides:
- **13+ Social Providers**: Facebook, Instagram, Twitter/X, LinkedIn, TikTok, YouTube, Pinterest, Reddit, Telegram, Mastodon, Discord, Slack, Webhooks
- **Media Processing**: Image/video conversions, thumbnails, CDN integration
- **Scheduling Engine**: Laravel Horizon-based queue with retry logic
- **OAuth Management**: Secure token storage and refresh mechanisms
- **Analytics Import**: Automated metrics collection from platforms
- **Template System**: Reusable post templates and variables
- **Team Features**: Workspaces, roles, permissions
- **API Layer**: RESTful endpoints for all operations

#### Core Components to Preserve
```php
// MixPost key abstractions to port
- SocialProvider abstract class (src/SocialProviders/)
- ServiceManager for external APIs (src/Services/)
- MediaHandler for file processing (src/Services/MediaHandler.php)
- PostScheduler for queue management (src/Jobs/)
- AccountManager for OAuth flows (src/Http/Controllers/AccountController.php)
- Import system for metrics (src/Jobs/ImportAccountData.php)
```

#### Database Migration Plan
```sql
-- MixPost tables to evolve
CREATE TABLE social_accounts (
    id UUID PRIMARY KEY,
    provider VARCHAR(50),
    credentials JSONB, -- Encrypted OAuth tokens
    metadata JSONB,
    -- New fields for enhanced features
    ai_settings JSONB,
    performance_metrics JSONB,
    custom_workflows JSONB
);

CREATE TABLE posts (
    id UUID PRIMARY KEY,
    content TEXT,
    media_ids UUID[],
    platforms JSONB,
    -- Enhanced scheduling
    optimal_time TIMESTAMP,
    ab_test_variants JSONB,
    ai_suggestions JSONB,
    performance_predictions JSONB
);

CREATE TABLE analytics (
    -- New time-series optimized structure
    time TIMESTAMPTZ NOT NULL,
    post_id UUID,
    platform VARCHAR(50),
    metrics JSONB,
    -- Hypertable for TimescaleDB
);
```

### Migration Implementation

#### Step 1: Service Layer Extraction
```typescript
// Port MixPost PHP services to TypeScript
interface SocialProvider {
  authenticate(): Promise<OAuthToken>;
  publish(post: Post): Promise<PublishResult>;
  fetchAnalytics(postId: string): Promise<Analytics>;
  getAccountInfo(): Promise<AccountInfo>;
}

class FacebookProvider implements SocialProvider {
  // Port from MixPost's Facebook/Meta provider
  async authenticate() {
    // OAuth flow implementation
  }
  
  async publish(post: Post) {
    // Graph API integration
  }
}

class TwitterProvider implements SocialProvider {
  // Port from MixPost's Twitter provider
  async authenticate() {
    // OAuth 2.0 implementation
  }
  
  async publish(post: Post) {
    // Twitter API v2 integration
  }
}
```

#### Step 2: Scheduling Engine Modernization
```typescript
// Replace Laravel Horizon with Bull
import Bull from 'bull';

class SocialScheduler {
  private queue: Bull.Queue;
  
  constructor() {
    this.queue = new Bull('social-posts', {
      redis: process.env.REDIS_URL,
      defaultJobOptions: {
        removeOnComplete: true,
        attempts: 3,
        backoff: {
          type: 'exponential',
          delay: 2000
        }
      }
    });
    
    this.setupProcessors();
  }
  
  async schedulePost(post: ScheduledPost) {
    // AI-enhanced scheduling
    const optimalTime = await this.predictOptimalTime(post);
    
    await this.queue.add('publish', post, {
      delay: optimalTime.getTime() - Date.now(),
      priority: post.priority
    });
  }
  
  private async predictOptimalTime(post: ScheduledPost) {
    // ML model for timing optimization
    const historicalData = await this.getHistoricalPerformance();
    const prediction = await this.aiService.predictBestTime({
      content: post.content,
      platform: post.platform,
      historicalData
    });
    
    return prediction.timestamp;
  }
}
```

#### Step 3: Modern UI Components
```typescript
// React components replacing MixPost Vue.js/Inertia
import { Calendar, Views } from 'react-big-calendar';
import { DndProvider } from 'react-dnd';

export function ContentCalendar() {
  const [posts, setPosts] = useState<ScheduledPost[]>([]);
  
  return (
    <DndProvider backend={HTML5Backend}>
      <Calendar
        localizer={localizer}
        events={posts}
        views={[Views.MONTH, Views.WEEK, Views.DAY]}
        onEventDrop={handleReschedule}
        components={{
          event: PostPreview,
          toolbar: CustomToolbar
        }}
      />
    </DndProvider>
  );
}

export function PostComposer() {
  return (
    <div className="grid grid-cols-2 gap-4">
      <div className="space-y-4">
        <RichTextEditor />
        <MediaUploader />
        <PlatformSelector />
        <AIAssistant />
      </div>
      <div className="space-y-4">
        <PlatformPreviews />
        <CharacterCounts />
        <HashtagSuggestions />
        <SchedulingOptions />
      </div>
    </div>
  );
}
```

### Integration Architecture

```mermaid
graph TB
    subgraph "Video Platform"
        VP[Video Processor]
        AI[AI Clipper]
        ED[Editor]
    end
    
    subgraph "Social Manager (Ex-MixPost)"
        SC[Scheduler Core]
        SP[Social Providers]
        AN[Analytics Engine]
        UI[Modern UI]
    end
    
    subgraph "Shared Services"
        DB[(PostgreSQL)]
        RD[(Redis)]
        S3[(S3/R2)]
        Q[Bull Queue]
    end
    
    VP --> SC
    AI --> SC
    ED --> SC
    SC --> SP
    SC --> Q
    SP --> DB
    AN --> DB
    UI --> SC
```

### Key Improvements Over MixPost

1. **Performance Optimizations**
   - Replace PHP with Node.js for better async handling
   - Use TimescaleDB for analytics (time-series optimization)
   - Implement Redis caching layers
   - Add CDN for media delivery

2. **AI Integration**
   - Content generation from video transcripts
   - Automated hashtag research
   - Engagement prediction models
   - A/B testing automation

3. **Modern Tech Stack**
   - TypeScript for type safety
   - React with Next.js for SSR
   - GraphQL for flexible API
   - WebSocket for real-time updates

4. **Enhanced Features**
   - Multi-workspace support
   - Advanced team collaboration
   - White-label capabilities
   - API-first architecture

### Migration Timeline

**Phase 1 (Week 1-2)**: Analysis & Planning
- Audit MixPost codebase
- Document all integrations
- Plan data migration strategy
- Design new architecture

**Phase 2 (Week 3-4)**: Core Services
- Port provider abstractions
- Implement authentication
- Build scheduling engine
- Set up queue system

**Phase 3 (Week 5-6)**: UI Development
- Create component library
- Build calendar interface
- Implement post composer
- Add analytics dashboard

**Phase 4 (Week 7-8)**: Integration & Testing
- Connect to video platform
- Migrate existing data
- Test all providers
- Performance optimization

### AI Comment System Implementation

#### Natural Language Post Refinement
```typescript
interface AICommentCommand {
  postId: string;
  userId: string;
  comment: string;
  timestamp: Date;
}

class PostRefinementAgent {
  private tools: AIToolkit;
  
  async processComment(command: AICommentCommand) {
    // Parse user intent from natural language
    const intent = await this.parseIntent(command.comment);
    
    switch(intent.action) {
      case 'rewrite':
        return await this.rewriteCaption(intent.params);
      case 'regenerate_image':
        return await this.generateNewImage(intent.params);
      case 'adjust_clip':
        return await this.modifyVideoClip(intent.params);
      case 'change_style':
        return await this.applyStyleChange(intent.params);
      case 'add_element':
        return await this.addContentElement(intent.params);
    }
  }
  
  private async rewriteCaption(params: RewriteParams) {
    const variations = await this.llm.generate({
      prompt: `Rewrite this caption: ${params.originalText}
               Style: ${params.style}
               Tone: ${params.tone}
               Length: ${params.targetLength}`,
      count: 3
    });
    
    return {
      type: 'caption_variations',
      options: variations,
      preview: true
    };
  }
  
  private async generateNewImage(params: ImageParams) {
    // Integration with DALL-E, Midjourney, or Stable Diffusion
    const newImage = await this.imageGen.create({
      prompt: params.description,
      style: params.style,
      dimensions: params.dimensions
    });
    
    return {
      type: 'new_image',
      url: newImage.url,
      alternatives: newImage.variations
    };
  }
}
```

#### Comment UI Integration
```typescript
export function PostCommentSystem({ post }: { post: ScheduledPost }) {
  const [comments, setComments] = useState<Comment[]>([]);
  const [preview, setPreview] = useState(post);
  const [isProcessing, setIsProcessing] = useState(false);
  
  const handleComment = async (text: string) => {
    setIsProcessing(true);
    
    // Send to AI agent
    const result = await aiAgent.processComment({
      postId: post.id,
      comment: text
    });
    
    // Update preview
    if (result.preview) {
      setPreview(result.updatedPost);
    }
    
    // Show options to user
    if (result.options) {
      showSelectionModal(result.options);
    }
    
    setIsProcessing(false);
  };
  
  return (
    <div className="flex gap-4">
      <div className="w-1/2">
        <PostPreview post={preview} />
        <CommentInput 
          onSubmit={handleComment}
          placeholder="Ask AI to change anything... 'Make it funnier', 'Add emoji', 'Shorten to 50 words'"
          suggestions={[
            "Make this more engaging",
            "Add a call-to-action",
            "Generate a new thumbnail",
            "Change the tone to professional",
            "Add trending hashtags"
          ]}
        />
        <CommentHistory comments={comments} />
      </div>
      <div className="w-1/2">
        <PlatformPreviews post={preview} />
        <VersionHistory versions={post.versions} />
      </div>
    </div>
  );
}
```

### Smart Lists & Auto-Routing System

#### Content Classification Engine
```typescript
interface ContentClassifier {
  analyzeContent(content: MediaContent): ContentProfile;
}

interface ContentProfile {
  type: 'video' | 'image' | 'text' | 'carousel' | 'story';
  dimensions: {
    width: number;
    height: number;
    aspectRatio: '16:9' | '9:16' | '1:1' | '4:5';
  };
  duration?: number;
  hasAudio?: boolean;
  textLength?: number;
  platforms: PlatformCompatibility[];
}

class SmartRouter {
  private rules: RoutingRule[];
  
  async routeContent(content: MediaContent): Promise<RoutingDecision> {
    const profile = await this.classifier.analyzeContent(content);
    
    // Apply routing rules based on content profile
    const eligiblePlatforms = this.determineEligiblePlatforms(profile);
    
    // Smart scheduling based on platform best practices
    const schedule = await this.optimizeSchedule(eligiblePlatforms, profile);
    
    return {
      platforms: eligiblePlatforms,
      schedule: schedule,
      reasoning: this.explainDecision(profile, eligiblePlatforms)
    };
  }
  
  private determineEligiblePlatforms(profile: ContentProfile): Platform[] {
    const platforms = [];
    
    // Video routing logic
    if (profile.type === 'video') {
      if (profile.aspectRatio === '9:16' && profile.duration <= 60) {
        platforms.push('tiktok', 'instagram_reels', 'youtube_shorts');
      }
      if (profile.aspectRatio === '16:9') {
        platforms.push('youtube', 'facebook', 'linkedin');
      }
      if (profile.duration <= 140) {
        platforms.push('twitter');
      }
    }
    
    // Image routing logic
    if (profile.type === 'image') {
      if (profile.aspectRatio === '1:1') {
        platforms.push('instagram_feed', 'facebook', 'linkedin');
      }
      if (profile.aspectRatio === '9:16') {
        platforms.push('instagram_stories', 'facebook_stories');
      }
      platforms.push('twitter', 'pinterest');
    }
    
    // Text-only routing
    if (profile.type === 'text' && profile.textLength <= 280) {
      platforms.push('twitter', 'mastodon');
    }
    
    return platforms;
  }
  
  private async optimizeSchedule(
    platforms: Platform[], 
    profile: ContentProfile
  ): Promise<Schedule[]> {
    const schedules = [];
    
    for (const platform of platforms) {
      const optimalTime = await this.ai.predictBestTime({
        platform,
        contentType: profile.type,
        historicalPerformance: await this.getHistoricalData(platform),
        audienceTimezone: await this.getAudienceTimezone(platform)
      });
      
      schedules.push({
        platform,
        scheduledTime: optimalTime,
        priority: this.calculatePriority(platform, profile)
      });
    }
    
    // Distribute posts to avoid flooding
    return this.distributeSchedule(schedules);
  }
  
  private distributeSchedule(schedules: Schedule[]): Schedule[] {
    const minInterval = 30; // minutes between posts
    let lastTime = new Date();
    
    return schedules
      .sort((a, b) => b.priority - a.priority)
      .map(schedule => {
        if (schedule.scheduledTime < lastTime) {
          schedule.scheduledTime = new Date(
            lastTime.getTime() + minInterval * 60000
          );
        }
        lastTime = schedule.scheduledTime;
        return schedule;
      });
  }
}
```

#### Smart List Configuration
```typescript
interface SmartList {
  id: string;
  name: string;
  rules: ListRule[];
  autoApprove: boolean;
  schedulePreferences: {
    timezone: string;
    activeHours: TimeRange[];
    avoidWeekends?: boolean;
    platformPriority: Record<Platform, number>;
  };
}

interface ListRule {
  condition: {
    field: 'type' | 'aspectRatio' | 'duration' | 'hashtags' | 'keywords';
    operator: 'equals' | 'contains' | 'lessThan' | 'greaterThan';
    value: any;
  };
  action: {
    platforms: Platform[];
    scheduling: 'immediate' | 'nextSlot' | 'optimal';
    modifications?: ContentModification[];
  };
}

// Example smart list configuration
const smartLists: SmartList[] = [
  {
    name: "Short-form Vertical Videos",
    rules: [{
      condition: {
        field: 'aspectRatio',
        operator: 'equals',
        value: '9:16'
      },
      action: {
        platforms: ['tiktok', 'instagram_reels', 'youtube_shorts'],
        scheduling: 'optimal',
        modifications: [{
          type: 'add_captions',
          style: 'trendy'
        }]
      }
    }],
    autoApprove: false,
    schedulePreferences: {
      timezone: 'America/New_York',
      activeHours: [
        { start: '08:00', end: '10:00' },
        { start: '12:00', end: '13:00' },
        { start: '17:00', end: '20:00' }
      ],
      platformPriority: {
        'tiktok': 1,
        'instagram_reels': 2,
        'youtube_shorts': 3
      }
    }
  },
  {
    name: "Professional Content",
    rules: [{
      condition: {
        field: 'keywords',
        operator: 'contains',
        value: ['business', 'professional', 'industry']
      },
      action: {
        platforms: ['linkedin', 'twitter'],
        scheduling: 'nextSlot',
        modifications: [{
          type: 'adjust_tone',
          value: 'professional'
        }]
      }
    }],
    autoApprove: true,
    schedulePreferences: {
      timezone: 'America/New_York',
      activeHours: [
        { start: '09:00', end: '17:00' }
      ],
      avoidWeekends: true
    }
  }
];
```

#### Bulk Review Interface
```typescript
export function SmartListReview() {
  const [pendingPosts, setPendingPosts] = useState<RoutedPost[]>([]);
  const [selectedPosts, setSelectedPosts] = useState<Set<string>>(new Set());
  
  return (
    <div className="space-y-6">
      <div className="flex justify-between items-center">
        <h2>Smart List Review Queue</h2>
        <div className="flex gap-2">
          <Button onClick={approveSelected}>
            Approve Selected ({selectedPosts.size})
          </Button>
          <Button onClick={approveAll} variant="primary">
            Approve All
          </Button>
        </div>
      </div>
      
      <div className="grid grid-cols-3 gap-4">
        {pendingPosts.map(post => (
          <PostCard
            key={post.id}
            post={post}
            selected={selectedPosts.has(post.id)}
            onSelect={(selected) => {
              if (selected) selectedPosts.add(post.id);
              else selectedPosts.delete(post.id);
              setSelectedPosts(new Set(selectedPosts));
            }}
          >
            <div className="mt-2 space-y-1">
              <div className="flex gap-2">
                {post.platforms.map(platform => (
                  <PlatformBadge key={platform} platform={platform} />
                ))}
              </div>
              <div className="text-sm text-gray-600">
                Scheduled: {formatDateTime(post.scheduledTime)}
              </div>
              <div className="flex gap-2">
                <Button size="sm" onClick={() => editSchedule(post)}>
                  Change Time
                </Button>
                <Button size="sm" onClick={() => editContent(post)}>
                  Edit Content
                </Button>
              </div>
            </div>
          </PostCard>
        ))}
      </div>
      
      <SmartListSettings 
        lists={smartLists}
        onUpdate={updateSmartList}
      />
    </div>
  );
}
```

### Unified Platform Benefits

#### Seamless Workflow
```yaml
workflow:
  1_record: "Native WebRTC recording with multiple participants"
  2_process: "AI clips generation with custom prompts"
  3_edit: "Browser-based video editing with templates"
  4_schedule: "Direct publishing without API calls"
  5_analyze: "Unified analytics across all content"
```

#### Data Advantages
- **Single Source of Truth**: All content and analytics in one database
- **Cross-Feature Intelligence**: Video performance informs scheduling
- **Unified User Profiles**: One account for all features
- **Integrated Analytics**: Video creation to social performance tracking

#### Technical Benefits
- **Shared Services**: One auth system, one queue, one storage
- **Consistent UI/UX**: Single design system across features
- **Reduced Complexity**: No webhook/API integration needed
- **Performance**: Direct database queries instead of API calls

#### Business Model Advantages
- **Complete Platform**: Compete with multiple tools (Opus, Riverside, Buffer, Hootsuite)
- **Higher Value**: Charge more for integrated solution
- **Stickier Product**: Harder to leave when all workflow is integrated
- **Data Moat**: Unique insights from end-to-end data

### Key Differentiating Features

#### 1. AI Comment System
**Problem Solved**: Tedious post editing and refinement process
- **Natural Language Commands**: "Make this funnier", "Add a CTA", "Shorten to 50 words"
- **Multi-Modal Editing**: Rewrite text, regenerate images, adjust video clips
- **Real-Time Preview**: See changes instantly before committing
- **Version Control**: Track all changes and revert if needed
- **AI Tools Available**:
  - Caption rewriting with tone/style control
  - Image generation via DALL-E/Stable Diffusion
  - Video clip adjustments
  - Hashtag and emoji suggestions
  - Platform-specific optimizations

#### 2. Smart Lists with Auto-Routing
**Problem Solved**: Manual platform selection and scheduling fatigue
- **Content-Aware Routing**: Automatically determines best platforms based on:
  - Aspect ratio (9:16 â†’ TikTok/Reels, 16:9 â†’ YouTube/LinkedIn)
  - Duration (â‰¤60s â†’ Shorts/Reels, >60s â†’ main feeds)
  - Content type (professional â†’ LinkedIn, casual â†’ Instagram)
  - Media format (video, image, text, carousel)
- **Intelligent Scheduling**:
  - AI predicts optimal posting times per platform
  - Automatically distributes posts to avoid flooding
  - Respects platform-specific best practices
  - Considers audience timezone and active hours
- **Bulk Review Interface**:
  - Review all auto-scheduled posts at once
  - Approve all or select specific posts
  - Quick time/platform adjustments
  - Set and forget with auto-approval rules
- **Custom Rules Engine**:
  - Create rules based on hashtags, keywords, content type
  - Set platform priorities and preferences
  - Define active hours and avoid weekends
  - Automatic content modifications per platform

## Next Steps

1. **Technical Validation**
   - Test Remotion with sample videos
   - Benchmark Whisper accuracy
   - Evaluate LLM performance
   - Validate FunClip components
   - Analyze MixPost codebase structure

2. **Infrastructure Setup**
   - Provision development environment
   - Set up CI/CD pipeline
   - Configure monitoring
   - Docker containerization

3. **MVP Development**
   - Start with single-track clipping
   - Implement basic prompt system
   - Create simple timeline editor
   - Test with real content

4. **User Testing**
   - Alpha testing with 10-20 users
   - Collect feedback on prompts
   - Refine AI accuracy
   - Iterate on UI/UX

5. **Production Readiness**
   - Security audit
   - Performance optimization
   - Documentation
   - White-label preparation

---

## Open-Source Libraries & Tools Research

### Analytics & Metrics Collection

#### Open Source Analytics Platforms

**Socioboard** - World's first open source Social Technology Enabler
- Supports 9+ social media networks
- Built-in analytics and reporting
- Fully customizable and extensible through plugins
- Available as web app and mobile apps
- GitHub: socioboard/Socioboard-5.0

**Social Media Analytics Libraries**
- **VADER** (4.5K GitHub stars) - Sentiment analysis for social media
  - Lexicon and rule-based approach
  - Handles emoticons, acronyms, slang
  - Excellent for informal social media text
- **TextBlob** - Beginner-friendly sentiment analysis
  - Easy API for sentiment classification
  - Good for smaller projects
- **Pattern** (8.2K GitHub stars) - Comprehensive Python solution
  - Web scrapers and API integration
  - Built-in sentiment analysis
- **NLP.js** (6K GitHub stars) - JavaScript sentiment analysis
  - Supports 40 languages
  - Real-time analysis capabilities
  - Great for social media monitoring

#### Analytics Dashboard Frameworks

**Metabase** (Best for user-friendly analytics)
- No SQL knowledge required
- Visual query builder
- 20+ data source integrations
- Self-service analytics
- Quick setup (< 5 minutes to first dashboard)
- Good for social media performance tracking

**Apache Superset** (Best for complex visualizations)
- 40+ pre-installed visualizations
- Geospatial visualization features
- SQL IDE included
- All features in open source version
- Best for data science teams

**Grafana** (Best for real-time monitoring)
- Excellent for time-series data
- Large plugin ecosystem
- Supports Prometheus, InfluxDB, Elasticsearch
- Highly customizable dashboards
- Great for live metrics tracking

### Ad Platform Integration

#### Official SDKs & Libraries

**Meta (Facebook/Instagram) Business SDK**
- **Python**: facebook-python-business-sdk
  - Official Meta Marketing API SDK
  - CRUD design pattern
  - Auto-generated API objects
  - Full Marketing API coverage
- **PHP**: facebook-php-business-sdk
  - Composer/Packagist distribution
  - PSR-4 autoloading
  - Implicit cursor fetching
- **Benefits in 2024**:
  - 40% cost reduction through automation
  - 3.2x higher ROAS on average
  - 58% reduction in cost per acquisition
  - 85% faster creative testing

**Google Ads API Client Libraries**
- **Official libraries**: Java, .NET, PHP, Python, Ruby, Perl
- **Python**: google-ads-python (GitHub)
  - OAuth2 authentication
  - Automatic paging for large datasets
  - Easy credential management
- **PHP**: google-ads-php (GitHub)
  - Composer distribution
  - v21 API support (2024)
- **Community libraries**:
  - Node.js: Opteo/google-ads-node
  - Go: Community-maintained options

**TikTok Ads API**
- REST API architecture
- Webhook support for event notifications
- Marketing API for programmatic management
- Python integration examples available
- Sandbox mode for testing (June 2024)
- Commercial Content API for creator marketplace

**LinkedIn Ads API**
- Campaign Manager API
- Lead generation capabilities
- Account-based marketing features
- Limited open source libraries available

#### Multi-Platform Ad Management

While no comprehensive open-source multi-platform ad management tools were found, the landscape shows:
- Most solutions are commercial (HubSpot, Madgicx, AdRoll)
- Focus on AI-powered automation in 2024
- Cross-channel integration is key
- Unified reporting dashboards essential

**Building Custom Solution Approach**:
1. Use official SDKs for each platform
2. Create abstraction layer for common operations
3. Build unified interface on top
4. Implement custom automation rules
5. Use open source dashboards for visualization

## Open-Source Libraries & Tools Research

### Video Processing Libraries

#### FFmpeg Ecosystem
- **FFmpeg.wasm**: Pure WebAssembly port of FFmpeg for browser-based video processing
  - Supports client-side transcoding without server infrastructure
  - File size limitation: 2GB due to WebAssembly constraints
  - Binary size: ~20MB (consider lazy loading)
  - Use cases: Format conversion, filtering, basic editing

- **fluent-ffmpeg**: Node.js wrapper for FFmpeg
  - Provides fluent API for complex FFmpeg operations
  - Good for server-side batch processing
  - Supports streaming and piping

#### Python Video Libraries
- **MoviePy 2.0**: Python library for programmatic video editing
  - Recently updated to v2.0 with breaking changes (2024)
  - Excellent for automation and bulk processing
  - Processing time typically less than video duration
  - Supports all FFmpeg formats

- **VidGear**: High-performance multi-threaded video processing
  - Built on OpenCV with threading optimizations
  - Excellent for multi-stream recording and processing
  - Supports RTSP, IP cameras, and streaming services
  - CamGear API for flexible camera handling

- **PySceneDetect**: Advanced scene detection library
  - Multiple detection algorithms (content, adaptive, threshold, hash)
  - Can automatically split videos at scene boundaries
  - Useful for identifying natural cut points for clips
  - CLI and Python API available

### AI/ML Tools

#### Transcription & Speech
- **OpenAI Whisper**: State-of-the-art speech recognition
  - Can run locally (no API costs) or via API
  - Whisper Large V3 and Turbo models available
  - WhisperLive for real-time transcription
  - Supports 100+ languages

- **ClipsAI**: Open-source Python library for automatic clipping
  - Automatically converts long videos into clips
  - Built specifically for social media content
  - Includes trimming and resizing capabilities

#### LLM Integration
- **LangChain**: Framework for LLM application development
  - Excellent for prompt management and chaining
  - Supports multiple LLM providers
  - Good for building complex AI workflows

### Web-Based Video Editors

#### React-Based Solutions
- **Remotion**: React framework for programmatic video creation
  - Version 4.0+ with improved performance
  - Requires licensing for commercial use
  - Excellent for template-based video generation
  - Large ecosystem and community

- **Reactive**: Simple React-based video editor
  - Open-source and free
  - Drag-and-drop interface
  - Basic editing capabilities

- **React Video Editor (DesignCombo)**: Canva/CapCut clone
  - Built with Remotion
  - Timeline-based editing
  - Good reference implementation

#### Canvas Libraries for Video UI
- **Fabric.js**: Powerful canvas library
  - fabric-video-editor project available on GitHub
  - Good for overlay and annotation features
  - Extensive object manipulation capabilities

- **Konva.js**: 2D canvas framework
  - Better performance than Fabric.js for complex scenes
  - React-Konva for React integration
  - Used in production video editors

### Video Player & Streaming

- **Video.js**: Industry-standard HTML5 video player
  - Extensive plugin ecosystem
  - Supports HLS, DASH streaming
  - Customizable UI and controls

- **Wavesurfer.js**: Audio waveform visualization
  - Essential for audio timeline display
  - Supports regions and markers
  - Good for precise audio editing

### Recording & WebRTC

- **RecordRTC**: Cross-browser audio/video recording
  - Works with MediaRecorder API
  - Supports multi-stream recording
  - Good fallback support

- **MediaStreamRecorder**: WebRTC recording library
  - Follows MediaRecorder API standards
  - Cross-browser support
  - Maintained and updated for 2024

- **Simple-peer**: Simplified WebRTC implementation
  - Good for P2P connections
  - Minimal configuration required
  - Works well for small-scale recording

### Specialized Tools

- **LosslessCut**: Cross-platform FFmpeg GUI
  - Excellent for lossless trimming
  - No re-encoding required
  - Fast operations on large files
  - Good reference for UI/UX

- **Auto-Editor**: Automatic video editing based on audio
  - Removes silence automatically
  - Good for podcast/tutorial editing
  - Python-based with CLI

### Frontend Frameworks & UI

- **Puck**: Open-source visual editor for React
  - Drag-and-drop interface builder
  - Good for template creation
  - Could be adapted for video templates

### Deployment & Infrastructure

- **FFmpeg-web**: Web UI for FFmpeg operations
  - Supports both browser and native
  - Good reference for UI patterns
  - Handles multiple format conversions

### Integration Considerations

#### For MVP (Milestone 1)
- Start with FFmpeg + fluent-ffmpeg for basic processing
- Use PySceneDetect for intelligent cut detection
- Implement Whisper locally for transcription
- Video.js for playback

#### For AI Features (Milestone 2)
- Add ClipsAI for automatic clipping logic
- Integrate LangChain for prompt management
- Use MoviePy for advanced processing

#### For Editor (Milestone 4)
- Choose between Remotion (powerful but licensed) or Fabric.js (free but more work)
- Add Konva.js for timeline UI
- Integrate Wavesurfer.js for audio visualization

---

## Analytics & Ad Platform Architecture

### Data Pipeline Architecture

```yaml
data_sources:
  organic_social:
    - Facebook Graph API
    - Instagram Basic Display API
    - Twitter API v2
    - LinkedIn Marketing API
    - TikTok Research API
    - YouTube Data API v3
  
  paid_advertising:
    - Meta Marketing API
    - Google Ads API
    - TikTok Marketing API
    - LinkedIn Ads API

ingestion:
  streaming: Apache Kafka / Pulsar
  batch: Apache Airflow
  
storage:
  time_series: TimescaleDB / InfluxDB
  analytics: ClickHouse / Apache Druid
  operational: PostgreSQL
  cache: Redis

processing:
  real_time: Apache Flink / Spark Streaming
  batch: Apache Spark
  ml_pipeline: Apache Beam
```

### Unified Analytics Schema

```sql
-- Core metrics table (time-series optimized)
CREATE TABLE social_metrics (
    time TIMESTAMPTZ NOT NULL,
    platform VARCHAR(50),
    account_id UUID,
    post_id VARCHAR(255),
    parent_video_id UUID,  -- Links clips to source
    metric_type VARCHAR(50),
    value NUMERIC,
    metadata JSONB,
    PRIMARY KEY (time, platform, post_id, metric_type)
);

-- Create hypertable for TimescaleDB
SELECT create_hypertable('social_metrics', 'time');

-- Ad campaign metrics
CREATE TABLE ad_metrics (
    time TIMESTAMPTZ NOT NULL,
    platform VARCHAR(50),
    campaign_id VARCHAR(255),
    ad_set_id VARCHAR(255),
    ad_id VARCHAR(255),
    metric_type VARCHAR(50),
    value NUMERIC,
    cost DECIMAL(10,2),
    conversions INT,
    PRIMARY KEY (time, platform, ad_id, metric_type)
);

-- Parent video to clips mapping
CREATE TABLE video_clip_analytics (
    parent_video_id UUID,
    clip_id UUID,
    platform VARCHAR(50),
    total_views BIGINT,
    total_engagement BIGINT,
    avg_watch_time FLOAT,
    performance_score FLOAT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

### Analytics API Design

```graphql
type Query {
  # Video and clip analytics
  videoAnalytics(videoId: ID!, dateRange: DateRange!): VideoAnalytics!
  clipPerformance(clipId: ID!): ClipMetrics!
  parentVideoMetrics(videoId: ID!): ParentVideoMetrics!
  
  # Platform analytics
  platformBreakdown(dateRange: DateRange!): [PlatformMetrics!]!
  crossPlatformComparison(postIds: [ID!]!): ComparisonReport!
  
  # Ad campaign analytics
  campaignPerformance(campaignId: ID!): CampaignMetrics!
  adSetAnalytics(adSetId: ID!): AdSetMetrics!
  roasReport(dateRange: DateRange!): ROASReport!
}

type VideoAnalytics {
  videoId: ID!
  totalClips: Int!
  platformBreakdown: [PlatformStats!]!
  aggregateMetrics: AggregateMetrics!
  topPerformingClips: [ClipMetrics!]!
  engagementTimeline: [TimeSeriesData!]!
}

type ParentVideoMetrics {
  videoId: ID!
  title: String!
  originalDuration: Float!
  totalClipsGenerated: Int!
  totalViewsAcrossClips: BigInt!
  totalEngagement: BigInt!
  platformDistribution: [PlatformStats!]!
  clipPerformanceHeatmap: [[Float!]!]!
  roi: ROIMetrics!
}
```

### Ad Campaign Management System

```typescript
// Unified ad platform abstraction
interface AdPlatform {
  createCampaign(config: CampaignConfig): Promise<Campaign>;
  updateBudget(campaignId: string, budget: Budget): Promise<void>;
  pauseCampaign(campaignId: string): Promise<void>;
  getMetrics(campaignId: string, dateRange: DateRange): Promise<Metrics>;
  createAudience(criteria: AudienceCriteria): Promise<Audience>;
}

class MetaAdsAdapter implements AdPlatform {
  private sdk: FacebookBusinessSDK;
  
  async createCampaign(config: CampaignConfig) {
    const campaign = await this.sdk.campaigns.create({
      name: config.name,
      objective: this.mapObjective(config.objective),
      budget_optimization: true,
      bid_strategy: config.bidStrategy
    });
    
    // Create ad sets from video clips
    for (const clip of config.clips) {
      await this.createAdSetFromClip(campaign.id, clip);
    }
    
    return this.normalizeCampaign(campaign);
  }
  
  private async createAdSetFromClip(campaignId: string, clip: VideoClip) {
    // Auto-generate ad creative from clip
    const creative = await this.generateCreative(clip);
    
    return this.sdk.adsets.create({
      campaign_id: campaignId,
      name: `${clip.title} - AdSet`,
      targeting: await this.optimizeTargeting(clip),
      creative: creative,
      optimization_goal: 'VIDEO_VIEWS'
    });
  }
}

class GoogleAdsAdapter implements AdPlatform {
  private client: GoogleAdsClient;
  
  async createCampaign(config: CampaignConfig) {
    const customer = this.client.Customer({
      customer_id: config.accountId
    });
    
    // Create campaign with video clips
    const operations = config.clips.map(clip => ({
      create: {
        name: `${config.name} - ${clip.title}`,
        advertising_channel_type: 'VIDEO',
        video_settings: {
          video_ad_format: this.getFormat(clip.aspectRatio)
        }
      }
    }));
    
    return customer.campaigns.mutate(operations);
  }
}

// Cross-platform campaign orchestrator
class CampaignOrchestrator {
  private platforms: Map<string, AdPlatform>;
  
  async launchMultiPlatformCampaign(config: MultiPlatformConfig) {
    const campaigns = [];
    
    // Distribute budget across platforms
    const budgetAllocation = this.optimizeBudgetAllocation(
      config.totalBudget,
      config.platforms,
      config.historicalPerformance
    );
    
    // Launch campaigns in parallel
    const promises = config.platforms.map(platform => {
      const adapter = this.platforms.get(platform);
      return adapter.createCampaign({
        ...config,
        budget: budgetAllocation[platform]
      });
    });
    
    return Promise.all(promises);
  }
  
  private optimizeBudgetAllocation(
    totalBudget: number,
    platforms: string[],
    historicalData: PerformanceData
  ): BudgetAllocation {
    // ML-based budget optimization
    const model = new BudgetOptimizationModel();
    return model.predict({
      budget: totalBudget,
      platforms,
      historicalROAS: historicalData.roas,
      audienceOverlap: historicalData.overlap
    });
  }
}
```

### Real-time Analytics Processing

```python
# Apache Flink job for real-time analytics
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

class SocialMediaAnalyticsJob:
    def __init__(self):
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.t_env = StreamTableEnvironment.create(self.env)
        
    def process_metrics_stream(self):
        # Define source from Kafka
        self.t_env.execute_sql("""
            CREATE TABLE social_events (
                platform STRING,
                post_id STRING,
                parent_video_id STRING,
                event_type STRING,
                value DOUBLE,
                timestamp TIMESTAMP(3),
                WATERMARK FOR timestamp AS timestamp - INTERVAL '5' SECOND
            ) WITH (
                'connector' = 'kafka',
                'topic' = 'social-metrics',
                'properties.bootstrap.servers' = 'localhost:9092',
                'format' = 'json'
            )
        """)
        
        # Aggregate metrics by parent video
        self.t_env.execute_sql("""
            CREATE VIEW parent_video_metrics AS
            SELECT 
                parent_video_id,
                platform,
                TUMBLE_START(timestamp, INTERVAL '1' HOUR) as window_start,
                COUNT(*) as event_count,
                SUM(CASE WHEN event_type = 'view' THEN value ELSE 0 END) as views,
                SUM(CASE WHEN event_type = 'engagement' THEN value ELSE 0 END) as engagement,
                AVG(CASE WHEN event_type = 'watch_time' THEN value ELSE NULL END) as avg_watch_time
            FROM social_events
            GROUP BY 
                parent_video_id,
                platform,
                TUMBLE(timestamp, INTERVAL '1' HOUR)
        """)
        
        # Write to TimescaleDB
        self.t_env.execute_sql("""
            INSERT INTO timescale_metrics
            SELECT * FROM parent_video_metrics
        """)
```

### Analytics Dashboard Components

```typescript
// React components for analytics visualization
export function ParentVideoAnalytics({ videoId }: { videoId: string }) {
  const { data, loading } = useQuery(GET_VIDEO_ANALYTICS, {
    variables: { videoId },
    pollInterval: 30000 // Update every 30 seconds
  });
  
  return (
    <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
      {/* Total metrics across all clips */}
      <MetricCard
        title="Total Views Across Clips"
        value={formatNumber(data?.totalViews)}
        change={data?.viewsChange}
        icon={<ViewsIcon />}
      />
      
      {/* Platform breakdown */}
      <PlatformBreakdownChart
        data={data?.platformBreakdown}
        title="Performance by Platform"
      />
      
      {/* Clip performance heatmap */}
      <ClipPerformanceHeatmap
        clips={data?.clips}
        metric="engagement_rate"
      />
      
      {/* Time series chart */}
      <TimeSeriesChart
        data={data?.timeline}
        metrics={['views', 'engagement', 'shares']}
        title="Performance Over Time"
      />
      
      {/* Top performing clips */}
      <TopClipsTable
        clips={data?.topClips}
        columns={['title', 'platform', 'views', 'engagement_rate', 'roi']}
      />
      
      {/* ROI Calculator */}
      <ROICalculator
        investment={data?.adSpend}
        returns={data?.revenue}
        organicValue={data?.organicReach}
      />
    </div>
  );
}

// Ad campaign dashboard
export function AdCampaignDashboard({ campaignId }: { campaignId: string }) {
  const { data } = useSubscription(CAMPAIGN_METRICS_SUBSCRIPTION, {
    variables: { campaignId }
  });
  
  return (
    <div className="space-y-6">
      {/* Cross-platform campaign overview */}
      <CampaignOverview
        platforms={data?.platforms}
        totalSpend={data?.totalSpend}
        totalConversions={data?.conversions}
        roas={data?.roas}
      />
      
      {/* Platform-specific performance */}
      <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
        {data?.platforms.map(platform => (
          <PlatformCampaignCard
            key={platform.name}
            platform={platform}
            metrics={platform.metrics}
            creatives={platform.creatives}
          />
        ))}
      </div>
      
      {/* A/B test results */}
      <ABTestResults
        tests={data?.abTests}
        winningVariants={data?.winners}
      />
      
      {/* Budget optimization recommendations */}
      <BudgetOptimizationPanel
        currentAllocation={data?.budgetAllocation}
        recommendations={data?.optimizationSuggestions}
        projectedImpact={data?.projectedROAS}
      />
    </div>
  );
}
```

## Technology Decision Matrix

### Backend Language Choice

| Criteria | Node.js/TypeScript | Python/FastAPI |
|----------|-------------------|----------------|
| **Video Processing** | FFmpeg.wasm, fluent-ffmpeg | MoviePy, VidGear, PySceneDetect |
| **AI/ML Integration** | Good (via APIs) | Excellent (native support) |
| **Real-time Features** | Excellent (Socket.io) | Good (WebSockets) |
| **MixPost Integration** | Natural fit (Laravel/PHP similar patterns) | Requires more bridging |
| **Performance** | Good for I/O, needs workers for CPU | Better for CPU-intensive tasks |
| **Ecosystem** | Large web ecosystem | Superior ML/video libraries |
| **Recommendation** | âœ… **Better for full-stack with recording** | Better for processing-heavy MVP |

### Video Editor Framework

| Framework | Pros | Cons | Best For |
|-----------|------|------|----------|
| **Remotion** | â€¢ Powerful and mature<br>â€¢ React-based<br>â€¢ Large community<br>â€¢ Template system | â€¢ Commercial license required<br>â€¢ Learning curve<br>â€¢ Heavy bundle size | Production-ready editor with templates |
| **Fabric.js + Custom** | â€¢ Free and open-source<br>â€¢ Flexible<br>â€¢ Lighter weight<br>â€¢ Full control | â€¢ More development time<br>â€¢ Need to build timeline<br>â€¢ Less video-specific features | Custom requirements, budget-conscious |
| **FFmpeg.wasm Only** | â€¢ No dependencies<br>â€¢ Runs in browser<br>â€¢ Free | â€¢ Limited to FFmpeg operations<br>â€¢ No visual editing<br>â€¢ 2GB file limit | Simple trimming and filters |
| **Recommendation** | Start with Fabric.js for MVP, consider Remotion for scale |

### AI/LLM Provider Strategy

| Provider | Cost | Performance | Privacy | Recommendation |
|----------|------|-------------|---------|----------------|
| **Whisper (Local)** | Free (after hardware) | Good with GPU | Excellent | âœ… **Best for transcription** |
| **LLaMA 3.1 (Local)** | Free (needs GPU) | Good | Excellent | âœ… **Best for cost at scale** |
| **Claude API** | $15/M tokens | Excellent | Good | Best for premium features |
| **GPT-4 API** | $30/M tokens | Excellent | Good | Alternative premium option |
| **Recommendation** | Hybrid: Local for base features, API for premium |

### Storage Architecture

| Solution | Cost | Scalability | Complexity | Best For |
|----------|------|-------------|------------|----------|
| **Local + S3** | Low | High | Medium | âœ… **Recommended approach** |
| **Pure S3** | Medium | Unlimited | Low | Large scale production |
| **Local Only** | Lowest | Limited | Lowest | Development/small scale |
| **Recommendation** | Local cache + S3 for persistence with lifecycle policies |

### Recording Technology

| Technology | Browser Support | Quality | Complexity | Recommendation |
|------------|----------------|---------|------------|----------------|
| **MediaRecorder + IndexedDB** | Excellent | Good | Low | âœ… **Best for MVP** |
| **WebRTC + MediaSoup** | Good | Excellent | High | Best for scale |
| **Simple-peer** | Good | Good | Medium | Good middle ground |
| **Recommendation** | Start with MediaRecorder, add MediaSoup for scale |

## API Design & Integration Points

### RESTful API Structure

```yaml
/api/v1:
  # Workflow Management
  /workflows:
    GET: List all workflows
    POST: Create new workflow
    /{id}:
      GET: Get workflow details
      PUT: Update workflow
      DELETE: Delete workflow
      /execute:
        POST: Manually trigger workflow
      /executions:
        GET: List execution history
        /{executionId}:
          GET: Get execution details
          /logs:
            GET: Get execution logs
  
  # Video Processing
  /videos:
    POST: Upload video for processing
    /{id}:
      GET: Get video metadata
      /clips:
        POST: Generate clips
        GET: List generated clips
      /transcript:
        GET: Get transcript
      /analyze:
        POST: Analyze video content
  
  # AI Operations
  /ai:
    /transcribe:
      POST: Transcribe audio/video
    /generate:
      POST: Generate content
    /analyze:
      POST: Analyze content
    /embeddings:
      POST: Generate embeddings
  
  # Webhook Management
  /webhooks:
    GET: List registered webhooks
    POST: Register new webhook
    /{id}:
      GET: Get webhook details
      PUT: Update webhook
      DELETE: Remove webhook
      /test:
        POST: Test webhook
  
  # Integration Endpoints
  /integrations:
    /mixpost:
      POST: Schedule post
      GET: Get scheduled posts
    /airtable:
      GET: Fetch records
      POST: Create record
      PUT: Update record
    /gdrive:
      POST: Upload file
      GET: List files
```

### Webhook Specifications

#### Incoming Webhooks (Triggers)
```typescript
interface IncomingWebhook {
  // Standard webhook handler
  path: string;
  method: 'POST' | 'GET';
  headers: {
    'X-Signature': string;  // HMAC signature
    'X-Timestamp': string;  // Request timestamp
    'X-Event-Type': string; // Event identifier
  };
  body: {
    event: string;
    data: any;
    metadata: {
      source: string;
      timestamp: string;
      correlation_id: string;
    };
  };
}

// Example: Video processing trigger
POST /webhooks/process-video
{
  "event": "video.uploaded",
  "data": {
    "video_url": "https://storage.example.com/video.mp4",
    "title": "My Video",
    "options": {
      "generate_clips": true,
      "add_captions": true,
      "platforms": ["instagram", "tiktok", "youtube"]
    }
  },
  "metadata": {
    "source": "airtable",
    "timestamp": "2024-01-13T10:00:00Z",
    "correlation_id": "abc-123"
  }
}
```

#### Outgoing Webhooks (Callbacks)
```typescript
interface OutgoingWebhook {
  url: string;
  method: 'POST';
  headers: {
    'Content-Type': 'application/json';
    'X-Service': 'video-clipper';
    'X-Signature': string;
  };
  body: {
    status: 'success' | 'error' | 'processing';
    data: any;
    error?: string;
    metadata: {
      execution_id: string;
      workflow_id: string;
      duration_ms: number;
      timestamp: string;
    };
  };
  retryPolicy: {
    maxAttempts: 3;
    backoffMs: [1000, 5000, 10000];
  };
}
```

### Integration Adapters

#### MixPost Integration
```typescript
class MixPostAdapter {
  async schedulePost(data: {
    content: string;
    media: MediaFile[];
    platforms: Platform[];
    schedule?: Date;
  }): Promise<PostResult> {
    // Direct integration with MixPost API
    return await this.api.post('/workspace/{workspace}/posts', {
      content: data.content,
      media_ids: await this.uploadMedia(data.media),
      accounts: await this.mapPlatforms(data.platforms),
      scheduled_at: data.schedule
    });
  }
  
  async uploadMedia(files: MediaFile[]): Promise<string[]> {
    // Upload to MixPost media library
    const uploads = files.map(file => 
      this.api.post('/workspace/{workspace}/media', file)
    );
    return Promise.all(uploads);
  }
}
```

#### Airtable Integration
```typescript
class AirtableAdapter {
  async syncContent(): Promise<void> {
    // Bidirectional sync with Airtable
    const records = await this.fetchRecords({
      filterByFormula: "Status = 'Ready'",
      sort: [{field: 'PublishDate', direction: 'asc'}]
    });
    
    for (const record of records) {
      await this.processRecord(record);
      await this.updateStatus(record.id, 'Processing');
    }
  }
  
  async webhook(data: AirtableWebhook): Promise<void> {
    // Handle Airtable automation webhooks
    if (data.action === 'record.created') {
      await this.triggerWorkflow('content-creation', data);
    }
  }
}
```

#### External Service Compatibility

```typescript
// N8N-compatible webhook format
interface N8NWebhook {
  headers: {
    'n8n-webhook-id': string;
    'n8n-workflow-id': string;
  };
  body: {
    // Maintain N8N structure for compatibility
    data: any;
    variables: Record<string, any>;
    action: {
      CALLBACK_WEBHOOK_URL: string;
      [key: string]: any;
    };
  };
}

// Zapier-compatible format
interface ZapierWebhook {
  headers: {
    'X-Hook-Secret': string;
  };
  body: {
    // Flat structure preferred by Zapier
    [key: string]: any;
  };
}

// Make.com (Integromat) format
interface MakeWebhook {
  headers: {
    'X-Make-Signature': string;
  };
  body: {
    // Nested structure support
    bundle: any;
    meta: any;
  };
}
```

### Event System

```typescript
// Internal event bus for workflow communication
class EventBus {
  events = new EventEmitter();
  
  // Workflow events
  on('workflow.started', (execution: Execution) => {});
  on('workflow.completed', (result: Result) => {});
  on('workflow.failed', (error: Error) => {});
  
  // Node events
  on('node.started', (node: Node, data: any) => {});
  on('node.completed', (node: Node, output: any) => {});
  on('node.failed', (node: Node, error: Error) => {});
  
  // Video processing events
  on('video.uploaded', (video: Video) => {});
  on('video.transcribed', (transcript: Transcript) => {});
  on('video.clipped', (clips: Clip[]) => {});
  on('video.published', (posts: Post[]) => {});
}
```

### GraphQL Alternative (Optional)

```graphql
type Query {
  workflows: [Workflow!]!
  workflow(id: ID!): Workflow
  executions(workflowId: ID!): [Execution!]!
  videos: [Video!]!
  video(id: ID!): Video
}

type Mutation {
  createWorkflow(input: WorkflowInput!): Workflow!
  executeWorkflow(id: ID!, input: JSON): Execution!
  uploadVideo(file: Upload!): Video!
  generateClips(videoId: ID!, options: ClipOptions!): [Clip!]!
  schedulePost(input: PostInput!): Post!
}

type Subscription {
  workflowExecution(id: ID!): ExecutionUpdate!
  videoProcessing(id: ID!): ProcessingUpdate!
}
```

## Detailed Technical Implementation

### WebRTC Recording Infrastructure

#### Recommended: OpenVidu
```javascript
// Backend: OpenVidu session creation
import { OpenVidu } from 'openvidu-node-client';

const openvidu = new OpenVidu(
  process.env.OPENVIDU_URL,
  process.env.OPENVIDU_SECRET
);

export async function createSession(sessionId) {
  const session = await openvidu.createSession({
    customSessionId: sessionId,
    recordingMode: 'INDIVIDUAL',
    defaultRecordingProperties: {
      outputMode: 'INDIVIDUAL',
      resolution: '1920x1080',
      frameRate: 30,
      recordingLayout: 'BEST_FIT'
    }
  });
  
  return session.sessionId;
}
```

#### Alternative: MediaSoup
- More flexible but requires more setup
- Better for custom requirements
- Lower level control
- Excellent performance

### Advanced Transcription Pipeline

#### WhisperX Integration
```python
# WhisperX for word-level timestamps
import whisperx
import gc

device = "cuda" 
audio_file = "audio.wav"
batch_size = 16
compute_type = "float16"

# 1. Transcribe with Whisper
model = whisperx.load_model("large-v2", device, compute_type=compute_type)
audio = whisperx.load_audio(audio_file)
result = model.transcribe(audio, batch_size=batch_size)

# 2. Align whisper output
model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)

# 3. Speaker diarization (optional)
diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)
diarize_segments = diarize_model(audio_file)
result = whisperx.assign_word_speakers(diarize_segments, result)
```

### Remotion Multi-Stream Composition

```javascript
// Remotion composition for multi-stream layouts
export const MultiStreamComposition = ({
  speakerVideo,
  screenShare,
  layout = 'pip' // pip, side-by-side, fullscreen-switch
}) => {
  const frame = useCurrentFrame();
  
  return (
    <AbsoluteFill>
      {layout === 'pip' && (
        <>
          <Video src={screenShare} />
          <div style={{
            position: 'absolute',
            bottom: 20,
            right: 20,
            width: '25%',
            borderRadius: 8,
            overflow: 'hidden',
            boxShadow: '0 4px 6px rgba(0,0,0,0.1)'
          }}>
            <Video src={speakerVideo} />
          </div>
        </>
      )}
      {layout === 'side-by-side' && (
        <div style={{ display: 'flex', height: '100%' }}>
          <Video src={speakerVideo} style={{ width: '50%' }} />
          <Video src={screenShare} style={{ width: '50%' }} />
        </div>
      )}
    </AbsoluteFill>
  );
};
```

### FFmpeg Advanced Filters

```bash
# Picture-in-Picture with shadow
ffmpeg -i screen.mp4 -i speaker.mp4 \
  -filter_complex "[1:v]scale=320:180[pip];[0:v][pip]overlay=main_w-overlay_w-10:main_h-overlay_h-10" \
  -c:a copy output.mp4

# Side-by-side with padding
ffmpeg -i speaker.mp4 -i screen.mp4 \
  -filter_complex "[0:v]scale=960:1080[left];[1:v]scale=960:1080[right];[left][right]hstack" \
  output.mp4

# Vertical stack for mobile
ffmpeg -i screen.mp4 -i speaker.mp4 \
  -filter_complex "[0:v]scale=1080:810[top];[1:v]scale=1080:1110[bottom];[top][bottom]vstack" \
  -s 1080x1920 output.mp4
```

### Video Quality Profiles

```javascript
const encodingProfiles = {
  social: {
    resolution: '1080x1920', // 9:16 for social
    bitrate: '4M',
    preset: 'fast',
    crf: 23,
    fps: 30
  },
  
  professional: {
    resolution: '1920x1080',
    bitrate: '8M',
    preset: 'slow',
    crf: 18,
    fps: 60
  },
  
  preview: {
    resolution: '640x360',
    bitrate: '800k',
    preset: 'ultrafast',
    crf: 28,
    fps: 24
  }
};

// Platform-specific optimization
const platformProfiles = {
  tiktok: { ...encodingProfiles.social, maxDuration: 60 },
  instagram: { ...encodingProfiles.social, maxDuration: 90 },
  youtube_shorts: { ...encodingProfiles.social, maxDuration: 60 },
  linkedin: { resolution: '1920x1080', bitrate: '5M', crf: 22 }
};
```

### Performance Optimization

#### Redis Caching Strategy
```javascript
import { Redis } from 'ioredis';
const redis = new Redis();

export async function getCachedTranscription(videoId) {
  const cached = await redis.get(`transcript:${videoId}`);
  if (cached) return JSON.parse(cached);
  
  const transcript = await generateTranscription(videoId);
  await redis.set(
    `transcript:${videoId}`, 
    JSON.stringify(transcript),
    'EX', 
    3600 // 1 hour cache
  );
  
  return transcript;
}

// Cache AI clip suggestions
export async function getCachedClipSuggestions(videoId, promptHash) {
  const key = `clips:${videoId}:${promptHash}`;
  const cached = await redis.get(key);
  if (cached) return JSON.parse(cached);
  
  const clips = await generateClipSuggestions(videoId, prompt);
  await redis.set(key, JSON.stringify(clips), 'EX', 7200);
  
  return clips;
}
```

#### Bull Queue Processing
```javascript
import Bull from 'bull';

const videoQueue = new Bull('video-processing', {
  redis: process.env.REDIS_URL,
  defaultJobOptions: {
    removeOnComplete: true,
    removeOnFail: false,
    attempts: 3,
    backoff: {
      type: 'exponential',
      delay: 2000
    }
  }
});

// Process video rendering jobs
videoQueue.process('render', 2, async (job) => {
  const { composition, props } = job.data;
  
  await renderMedia({
    composition,
    serveUrl: process.env.REMOTION_URL,
    codec: 'h264',
    outputLocation: `s3://bucket/renders/${job.id}.mp4`,
    inputProps: props,
    onProgress: (progress) => {
      job.progress(progress.percent);
    }
  });
  
  return { success: true, url: `${job.id}.mp4` };
});

// Process transcription jobs
videoQueue.process('transcribe', async (job) => {
  const { videoPath, language } = job.data;
  const result = await transcribeWithWhisper(videoPath, language);
  return result;
});
```

## Quick Start Guide

### Option A: Node.js/TypeScript Stack (Recommended)

```bash
# Initialize project
npx create-next-app@latest video-clipper --typescript --tailwind --app
cd video-clipper

# Install core dependencies
npm install express socket.io fluent-ffmpeg multer bull
npm install @ffmpeg/ffmpeg @ffmpeg/core # FFmpeg.wasm
npm install video.js wavesurfer.js fabric konva react-konva
npm install langchain openai whisper

# Database and storage
npm install prisma @prisma/client redis ioredis
npm install @aws-sdk/client-s3 multer-s3

# Development tools
npm install -D @types/node nodemon tsx
```

### Option B: Python/FastAPI Stack

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install core dependencies
pip install fastapi uvicorn python-multipart
pip install moviepy vidgear opencv-python-headless
pip install scenedetect[opencv] whisper transformers
pip install langchain openai anthropic

# Database and storage
pip install sqlalchemy alembic psycopg2-binary redis
pip install boto3 pillow

# Development tools
pip install black pytest python-dotenv
```

### Docker Compose Setup

```yaml
version: '3.8'
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: videoclipper
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

volumes:
  postgres_data:
  minio_data:
```

### Environment Variables Template

```env
# Database
DATABASE_URL=postgresql://admin:password@localhost:5432/videoclipper

# Redis
REDIS_URL=redis://localhost:6379

# Storage (S3/MinIO)
S3_ENDPOINT=http://localhost:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_BUCKET=videos

# AI Services
OPENAI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
WHISPER_MODEL_PATH=/models/whisper-large-v3

# MixPost Integration
MIXPOST_API_URL=http://your-mixpost-instance.com/api
MIXPOST_API_TOKEN=your_token_here
MIXPOST_WORKSPACE_ID=your_workspace_id

# App Configuration
NODE_ENV=development
PORT=3000
SECRET_KEY=your_secret_key_here
```

### First Steps After Setup

1. **Test FFmpeg Installation**
   ```bash
   ffmpeg -version
   # If not installed: brew install ffmpeg (Mac) or apt install ffmpeg (Linux)
   ```

2. **Initialize Database**
   ```bash
   # For Node.js
   npx prisma init
   npx prisma migrate dev

   # For Python
   alembic init alembic
   alembic revision --autogenerate -m "Initial migration"
   alembic upgrade head
   ```

3. **Test Video Processing**
   ```javascript
   // Node.js test
   const ffmpeg = require('fluent-ffmpeg');
   
   ffmpeg('input.mp4')
     .output('output.mp4')
     .size('1280x720')
     .on('end', () => console.log('Processing finished'))
     .run();
   ```

   ```python
   # Python test
   from moviepy.editor import VideoFileClip
   
   clip = VideoFileClip("input.mp4")
   clip.resize((1280, 720)).write_videofile("output.mp4")
   ```

4. **Test MixPost Connection**
   ```bash
   curl -H "Authorization: Bearer YOUR_TOKEN" \
        https://your-mixpost.com/api/workspace/YOUR_WORKSPACE/accounts
   ```

## Appendix

### Useful Resources
- [Remotion Documentation](https://www.remotion.dev/docs/)
- [Whisper API Guide](https://github.com/openai/whisper)
- [FFmpeg Best Practices](https://ffmpeg.org/documentation.html)
- [WebRTC Implementation](https://webrtc.org/getting-started/overview)
- [MixPost API Docs](https://mixpost.pro/docs)
- [MoviePy Guide](https://zulko.github.io/moviepy/)
- [PySceneDetect Docs](https://www.scenedetect.com/)
- [Fabric.js Demos](http://fabricjs.com/demos/)
- [Konva.js Examples](https://konvajs.org/docs/sandbox/)

### Estimated Costs (Monthly)
- **Infrastructure**: $500-2000 (depending on scale)
- **AI APIs**: $200-1000 (or self-host)
- **Storage**: $100-500
- **CDN**: $50-200
- **Total**: $850-3700/month

---

*Document Version: 1.0*  
*Last Updated: January 2025*  
*Created for: Custom Opus Clips Alternative Project*